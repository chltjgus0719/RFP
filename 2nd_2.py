## ì¶œì²˜ì— ë¹„ë”©ë„˜ë²„ ëœ¨ë„ë¡... ì˜¤í›„ë¶€í„° ìˆ˜ì •ì¤‘ì¸ íŒŒì¼



import os
import json
import shutil
import pandas as pd
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from tqdm import tqdm
import tiktoken
import gradio as gr

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì„¤ì •
folder_path = "sdf_sample"
index_dir = "faiss_index"
processed_files_path = os.path.join(index_dir, "processed_files.json")
faiss_index_file = os.path.join(index_dir, "index.faiss")
# ì½”ë“œ ë²„ì „ ê´€ë¦¬
CODE_VERSION = 2

# TSV íŒŒì¼ ìë™ ë³‘í•©
tsv_files = [f for f in os.listdir(folder_path) if f.endswith('.tsv')]
if len(tsv_files) < 2:
    raise FileNotFoundError(f"í´ë” '{folder_path}'ì— ìµœì†Œ 2ê°œì˜ TSV íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {tsv_files}")
file1, file2 = tsv_files[:2]
df1 = pd.read_csv(os.path.join(folder_path, file1), sep="\t", encoding="utf-8")
df2 = pd.read_csv(os.path.join(folder_path, file2), sep="\t", encoding="utf-8")
for df in (df1, df2):
    df.columns = df.columns.str.strip()
    df['bidding_info_id'] = df['bidding_info_id'].astype(str).str.strip()
merged_df = pd.merge(df1, df2, on="bidding_info_id", how="left", suffixes=("_1","_2"))
merged_docs = [
    Document(
        page_content='\n'.join(f"{col}: {row[col]}" for col in merged_df.columns),
        metadata={
            "bidding_info": row['bidding_info_id'],
            "source": f"{folder_path}/{file1}&{folder_path}/{file2}"
        }
    )
    for _, row in merged_df.iterrows()
]

# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ë° í† í° ìˆ˜ ì¶”ì •
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
encoding = tiktoken.get_encoding("o200k_base")

def estimate_tokens(text):
    return len(encoding.encode(text))

def group_batches(docs, limit=150_000):
    batches, curr, cnt = [], [], 0
    for d in docs:
        t = estimate_tokens(d.page_content)
        if cnt + t > limit:
            batches.append(curr)
            curr, cnt = [d], t
        else:
            curr.append(d)
            cnt += t
    if curr:
        batches.append(curr)
    return batches

# ì„ë² ë”© ê°ì²´ ìƒì„±
embedding = OpenAIEmbeddings()

# ìºì‹œ ë¬´íš¨í™” ë° ì¬ë¹Œë“œ ì¡°ê±´
rebuild = True
if os.path.exists(processed_files_path):
    with open(processed_files_path, 'r', encoding='utf-8') as f:
        meta = json.load(f)
    rebuild = meta.get('code_version') != CODE_VERSION
if rebuild and os.path.exists(index_dir):
    shutil.rmtree(index_dir)

# ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”
if os.path.exists(index_dir) and os.path.exists(faiss_index_file):
    vectordb = FAISS.load_local(index_dir, embedding, allow_dangerous_deserialization=True)
    print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ")
else:
    os.makedirs(index_dir, exist_ok=True)
    vectordb = None
    print("âœ… ì¸ë±ìŠ¤ ì´ˆê¸°í™”(ìºì‹œ ì—†ìŒ)")

# ì„ë² ë”© ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
if vectordb is None:
    batches = group_batches(splitter.split_documents(merged_docs))
    vectordb = FAISS.from_documents(batches[0], embedding)
    for b in tqdm(batches[1:], desc="ğŸ” ì¶”ê°€ ì„ë² ë”©"):
        vectordb.add_documents(b)
    vectordb.save_local(index_dir)
    # ë©”íƒ€ ê¸°ë¡
    with open(processed_files_path,'w',encoding='utf-8') as f:
        json.dump({"code_version": CODE_VERSION}, f, ensure_ascii=False, indent=2)
    print("âœ… ì„ë² ë”© ì™„ë£Œ ë° ì €ì¥")

# PromptTemplate ì •ì˜
template = """
[ì‹œìŠ¤í…œ ì§€ì¹¨ ì‹œì‘]
1) source_documentsê°€ 1ê°œ ì´ìƒ ì¡°íšŒë˜ë©´ <details> í† ê¸€ í¬í•¨
2) source_documentsê°€ 0ê°œ ì¡°íšŒë˜ë©´ <details> ë¸”ë¡ ì ˆëŒ€ ìƒëµ
3) ë¹ˆ <details> ë¸”ë¡ì´ ìƒì„±ë˜ì—ˆê±°ë‚˜ ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ë¸”ë¡ ì „ì²´ ì‚­ì œ
[ì‹œìŠ¤í…œ ì§€ì¹¨ ë]

ë‹¤ìŒ ë³‘í•©ëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.

{context}

ì§ˆë¬¸: {question}

== ì¶œë ¥ í˜•ì‹ ==
ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸: <ê°’>
ìš”êµ¬ì‚¬í•­ëª…: <ê°’>
ìš”êµ¬ì‚¬í•­ ë¶„ë¥˜: <ê°’>
ìš”êµ¬ì‚¬í•­ ì •ì˜: <ê°’>
ìš”êµ¬ì‚¬í•­ ì„¸ë¶€ë‚´ìš©: <ê°’>
ê´€ë ¨ ìš”êµ¬ì‚¬í•­: <ê°’>
ì‚°ì¶œ ì •ë³´: <ê°’>
"""
formatted_prompt = PromptTemplate(input_variables=["context","question"], template=template)

# ì²´ì¸ ìƒì„± í•¨ìˆ˜
def create_chain(use_format=False):
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})
    return ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(model_name="gpt-4.1-mini", temperature=0),
        retriever=retriever,
        memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer"),
        return_source_documents=True,
        verbose=False,
        **({"combine_docs_chain_kwargs": {"prompt": formatted_prompt}} if use_format else {})
    )

# ì²´ì¸ ì¸ìŠ¤í„´ìŠ¤
default_chain = create_chain(use_format=False)
formatted_chain = create_chain(use_format=True)

# Gradio ì±—ë´‡ í•¨ìˆ˜
def chatbot_fn(message, history):
    history = history or []
    if "ì¶œë ¥" in message:
        result = formatted_chain.invoke({"question": message})
        answer = result.get("answer", "")
        docs = [d for d in result.get("source_documents", []) if d.metadata.get("bidding_info")]
        if docs:
            sources = "\n".join(f"- {d.metadata['bidding_info']}" for d in docs)
            wrapped = f"<details><summary>ì¶œì²˜ ë‚´ìš© ë³´ê¸°</summary>\n{sources}\n</details>"
            answer = f"{answer}\n{wrapped}"
    else:
        result = default_chain.invoke({"question": message})
        answer = result.get("answer", "")
    history.append((message, answer))
    return history, history

with gr.Blocks() as demo:
    state = gr.State([])
    chat_display = gr.Chatbot()
    txt = gr.Textbox(placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
    txt.submit(chatbot_fn, [txt, state], [chat_display, state])

demo.launch(inbrowser=True)
