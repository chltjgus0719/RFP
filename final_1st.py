import os
import json
import shutil
import pandas as pd
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from tqdm import tqdm
import tiktoken
import gradio as gr

# í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
load_dotenv()
folder_path = "C:\\Users\\gram\\Desktop\\langchain_trytry\\sdf_sample"

# â”€â”€â”€ ì—¬ê¸°ë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë°”ê¾¸ì—ˆìŠµë‹ˆë‹¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
index_dir = "C:\\Users\\gram\\Desktop\\langchain_trytry\\faiss_index"
processed_files_path = os.path.join(index_dir, "processed_files.json")
faiss_index_file = os.path.join(index_dir, "index.faiss")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CODE_VERSION = 2

# TSV íŒŒì¼ ìë™ ë³‘í•©
tsv_files = [f for f in os.listdir(folder_path) if f.endswith('.tsv')]
if len(tsv_files) < 2:
    raise FileNotFoundError(f"í´ë” '{folder_path}'ì— ìµœì†Œ 2ê°œì˜ TSV íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {tsv_files}")
file1, file2 = tsv_files[:2]

# â”€â”€â”€ ì—¬ê¸°ì— ë³‘í•© ë¡œì§ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df1 = pd.read_csv(os.path.join(folder_path, file1), sep="\t", encoding="utf-8")
df2 = pd.read_csv(os.path.join(folder_path, file2), sep="\t", encoding="utf-8")
for df in (df1, df2):
    df.columns = df.columns.str.strip()
    df['bidding_info_id'] = df['bidding_info_id'].astype(str).str.strip()
merged_df = pd.merge(df1, df2, on="bidding_info_id", how="left", suffixes=("_1", "_2"))
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

merged_docs = [
    Document(
        page_content="\n".join(f"{col}: {row[col]}" for col in merged_df.columns),
        metadata={"bidding_info": row['bidding_info_id']}
    )
    for _, row in merged_df.iterrows()
]

# í† í° ë° ë°°ì¹˜ ê·¸ë£¹í™” ì„¤ì •
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
encoding = tiktoken.get_encoding("o200k_base")
def estimate_tokens(text): return len(encoding.encode(text))
def group_batches(docs, limit=150_000):
    batches, curr, cnt = [], [], 0
    for d in docs:
        t = estimate_tokens(d.page_content)
        if cnt + t > limit:
            batches.append(curr); curr, cnt = [d], t
        else:
            curr.append(d); cnt += t
    if curr: batches.append(curr)
    return batches

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ/ë¹Œë“œ
embedding = OpenAIEmbeddings()

# â”€â”€â”€ ì¬ë¹Œë“œ íŒë‹¨ ë¡œì§ì„ â€œë©”íƒ€+íƒ€ì„ìŠ¤íƒ¬í”„â€ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (1) ì›ë³¸ TSV íŒŒì¼ë“¤ ì¤‘ ê°€ì¥ ìµœê·¼ ìˆ˜ì • ì‹œê°„ì„ êµ¬í•¨
tsv_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tsv')]
latest_data_ts = max(os.path.getmtime(p) for p in tsv_paths)

# (2) processed_files.jsonì´ ìˆìœ¼ë©´, "code_version"ê³¼ "data_timestamp"ë¥¼ ì½ì–´ ë¹„êµ
if os.path.exists(processed_files_path):
    with open(processed_files_path, 'r', encoding='utf-8') as f:
        meta = json.load(f)
    saved_code_ver = meta.get("code_version", -1)
    saved_data_ts  = meta.get("data_timestamp", -1)
    rebuild = (saved_code_ver != CODE_VERSION) or (saved_data_ts != latest_data_ts)
else:
    # ë©”íƒ€ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì‚­ì œí•˜ì§€ ì•Šê³  ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€
    rebuild = False

# (3) ì¬ë¹Œë“œê°€ í•„ìš”í•  ë•Œë§Œ index_dir í´ë”ë¥¼ ì‚­ì œ
if rebuild and os.path.exists(index_dir):
    shutil.rmtree(index_dir)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if os.path.exists(index_dir) and os.path.exists(faiss_index_file):
    vectordb = FAISS.load_local(index_dir, embedding, allow_dangerous_deserialization=True)
    print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ")
else:
    os.makedirs(index_dir, exist_ok=True)
    vectordb = None
    print("âœ… ì¸ë±ìŠ¤ ì´ˆê¸°í™”(ìºì‹œ ì—†ìŒ)")

if vectordb is None:
    batches = group_batches(splitter.split_documents(merged_docs))
    vectordb = FAISS.from_documents(batches[0], embedding)
    for b in tqdm(batches[1:], desc="ğŸ” ì¶”ê°€ ì„ë² ë”©"):
        vectordb.add_documents(b)
    vectordb.save_local(index_dir)
    # â”€â”€â”€ ë©”íƒ€ ê¸°ë¡ ì‹œ data_timestampê¹Œì§€ í•¨ê»˜ ì €ì¥í•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with open(processed_files_path, 'w', encoding='utf-8') as f:
        json.dump({
            "code_version": CODE_VERSION,
            "data_timestamp": latest_data_ts
        }, f, ensure_ascii=False, indent=2)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("âœ… ì„ë² ë”© ì™„ë£Œ ë° ì €ì¥")

# PromptTemplate ì •ì˜
template = """
[ì‹œìŠ¤í…œ ì§€ì¹¨]
1) source_documents>0 â†’ <details> í† ê¸€ í¬í•¨
2) source_documents=0 â†’ í† ê¸€ ë¸”ë¡ ìƒëµ
3) ë¹ˆ í† ê¸€ ë°œìƒ ì‹œ ì œê±°
[ë]

{context}

ì§ˆë¬¸: {question}

==ì¶œë ¥ í˜•ì‹==
ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸: <ê°’>
ìš”êµ¬ì‚¬í•­ëª…: <ê°’>
ìš”êµ¬ì‚¬í•­ ë¶„ë¥˜: <ê°’>
ìš”êµ¬ì‚¬í•­ ì •ì˜: <ê°’>
ìš”êµ¬ì‚¬í•­ ì„¸ë¶€ë‚´ìš©: <ê°’>
ê´€ë ¨ ìš”êµ¬ì‚¬í•­: <ê°’>
ì‚°ì¶œ ì •ë³´: <ê°’>
"""
formatted_prompt = PromptTemplate(input_variables=["context","question"], template=template)

# ì²´ì¸ ìƒì„± í•¨ìˆ˜
def create_chain(use_format=False):
    retriever = vectordb.as_retriever(search_kwargs={"k":3})
    return ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(model_name="gpt-4.1-mini", temperature=0),
        retriever=retriever,
        memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer"),
        return_source_documents=True,
        verbose=False,
        **({"combine_docs_chain_kwargs":{"prompt":formatted_prompt}} if use_format else {})
    )
default_chain = create_chain(False)
formatted_chain = create_chain(True)

# Gradio ì±—ë´‡ í•¨ìˆ˜ (í† ê¸€ ë¡œì§ í¬í•¨)
def chatbot_fn(message, state):
    state = state or []
    state.append({"role":"user","content":message})
    chain = formatted_chain if "ì¶œë ¥" in message else default_chain
    result = chain.invoke({"question":message})
    answer = result.get("answer","")

    # ì¶œì²˜ í† ê¸€ ì¶”ê°€
    docs = result.get("source_documents", [])
    docs = [d for d in docs if d.metadata.get("bidding_info")]
    if docs:
        sources = "\n".join(f"- {d.metadata['bidding_info']}" for d in docs)
        answer += f"\n<details><summary>ì¶œì²˜ ë‚´ìš© ë³´ê¸°</summary>\n{sources}\n</details>"

    state.append({"role":"assistant","content":answer})
    return state, state, ""

# Gradio UI ë¸”ë¡
data_state = gr.State([])
with gr.Blocks() as demo:
    chat_display = gr.Chatbot(height=700, type="messages")
    txt = gr.Textbox(placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
    txt.submit(
        chatbot_fn,
        inputs=[txt, data_state],
        outputs=[chat_display, data_state, txt]
    )

demo.launch(inbrowser=True)
