import os
import json
import shutil
import pandas as pd
from dotenv import load_dotenv

# ì›ë˜ ì“°ì‹œë˜ íŒ¨í‚¤ì§€ ê²½ë¡œë¡œ ë³µì›
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate

from tqdm import tqdm
import tiktoken
import gradio as gr

# â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ ë° ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
folder_path = "C:\\Users\\gram\\Desktop\\langchain_trytry\\sdf_sample"

index_dir = "C:\\Users\\gram\\Desktop\\langchain_trytry\\faiss_index"
processed_files_path = os.path.join(index_dir, "processed_files.json")
faiss_index_file = os.path.join(index_dir, "index.faiss")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CODE_VERSION = 3

# TSV íŒŒì¼ ìë™ ë³‘í•©
tsv_files = [f for f in os.listdir(folder_path) if f.endswith('.tsv')]
if len(tsv_files) < 2:
    raise FileNotFoundError(f"í´ë” '{folder_path}'ì— ìµœì†Œ 2ê°œì˜ TSV íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {tsv_files}")
file1, file2 = tsv_files[:2]

# â”€â”€â”€ ë³‘í•© ë¡œì§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df1 = pd.read_csv(os.path.join(folder_path, file1), sep="\t", encoding="utf-8")
df2 = pd.read_csv(os.path.join(folder_path, file2), sep="\t", encoding="utf-8")
for df in (df1, df2):
    df.columns = df.columns.str.strip()
    df['bidding_info_id'] = df['bidding_info_id'].astype(str).str.strip()

merged_df = pd.merge(df1, df2, on="bidding_info_id", how="left", suffixes=("_1", "_2"))
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ê° í–‰ì„ Document ê°ì²´ë¡œ ë³€í™˜
merged_docs = [
    Document(
        page_content="\n".join(f"{col}: {row[col]}" for col in merged_df.columns),
        metadata={"bidding_info": row['bidding_info_id']}
    )
    for _, row in merged_df.iterrows()
]

# í† í° ìˆ˜ ê³„ì‚° ë° ë°°ì¹˜ ê·¸ë£¹í™”

# â”€â”€â”€ í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (ìˆ˜ì •ëœ ë¶€ë¶„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    separators=[
        "\n"  # ì—°ì†ëœ ë¹ˆ ì¤„(ë‹¨ë½ êµ¬ë¶„)ìœ¼ë¡œë§Œ ë¶„í• 
    ]
)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
encoding = tiktoken.get_encoding("o200k_base")  # ì›ë˜ëŒ€ë¡œ "o200k_base" ì‚¬ìš©
def estimate_tokens(text):
    return len(encoding.encode(text))

def group_batches(docs, limit=150_000):
    batches, curr, cnt = [], [], 0
    for d in docs:
        t = estimate_tokens(d.page_content)
        if cnt + t > limit:
            batches.append(curr)
            curr, cnt = [d], t
        else:
            curr.append(d)
            cnt += t
    if curr:
        batches.append(curr)
    return batches

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ/ë¹Œë“œ
embedding = OpenAIEmbeddings()

# â”€â”€â”€ ì¬ë¹Œë“œ íŒë‹¨ ë¡œì§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tsv_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tsv')]
latest_data_ts = max(os.path.getmtime(p) for p in tsv_paths)

if os.path.exists(processed_files_path):
    with open(processed_files_path, 'r', encoding='utf-8') as f:
        meta = json.load(f)
    saved_code_ver = meta.get("code_version", -1)
    saved_data_ts  = meta.get("data_timestamp", -1)
    rebuild = (saved_code_ver != CODE_VERSION) or (saved_data_ts != latest_data_ts)
else:
    rebuild = False

if rebuild and os.path.exists(index_dir):
    shutil.rmtree(index_dir)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if os.path.exists(index_dir) and os.path.exists(faiss_index_file):
    vectordb = FAISS.load_local(index_dir, embedding, allow_dangerous_deserialization=True)
    print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ")
else:
    os.makedirs(index_dir, exist_ok=True)
    vectordb = None
    print("âœ… ì¸ë±ìŠ¤ ì´ˆê¸°í™”(ìºì‹œ ì—†ìŒ)")

if vectordb is None:
    batches = group_batches(splitter.split_documents(merged_docs))
    vectordb = FAISS.from_documents(batches[0], embedding)
    for b in tqdm(batches[1:], desc="ğŸ” ì¶”ê°€ ì„ë² ë”©"):
        vectordb.add_documents(b)
    vectordb.save_local(index_dir)
    with open(processed_files_path, 'w', encoding='utf-8') as f:
        json.dump({
            "code_version": CODE_VERSION,
            "data_timestamp": latest_data_ts
        }, f, ensure_ascii=False, indent=2)
    print("âœ… ì„ë² ë”© ì™„ë£Œ ë° ì €ì¥")

# PromptTemplate ì •ì˜
template = """
[ì‹œìŠ¤í…œ ì§€ì¹¨]
1) source_documents>0 â†’ <details> í† ê¸€ í¬í•¨
2) source_documents=0 â†’ í† ê¸€ ë¸”ë¡ ìƒëµ
3) ë¹ˆ í† ê¸€ ë°œìƒ ì‹œ ì œê±°
[ë]

{context}

ì§ˆë¬¸: {question}

==ì¶œë ¥ í˜•ì‹==
ìš”êµ¬ì‚¬í•­ ê³ ìœ ë²ˆí˜¸: <ê°’>
ìš”êµ¬ì‚¬í•­ëª…: <ê°’>
ìš”êµ¬ì‚¬í•­ ë¶„ë¥˜: <ê°’>
ìš”êµ¬ì‚¬í•­ ì •ì˜: <ê°’>
ìš”êµ¬ì‚¬í•­ ì„¸ë¶€ë‚´ìš©: <ê°’>
ê´€ë ¨ ìš”êµ¬ì‚¬í•­: <ê°’>
ì‚°ì¶œ ì •ë³´: <ê°’>
"""
formatted_prompt = PromptTemplate(input_variables=["context","question"], template=template)

# ì²´ì¸ ìƒì„± í•¨ìˆ˜
def create_chain(use_format=False):
    retriever = vectordb.as_retriever(search_kwargs={"k":3})
    return ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(model_name="gpt-4.1-mini", temperature=0),
        retriever=retriever,
        memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer"),
        return_source_documents=True,
        verbose=False,
        **({"combine_docs_chain_kwargs":{"prompt":formatted_prompt}} if use_format else {})
    )

default_chain = create_chain(False)
formatted_chain = create_chain(True)


# â”€â”€â”€ Gradio UI ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with gr.Blocks() as demo:
    # State ì»´í¬ë„ŒíŠ¸ë¥¼ ë°˜ë“œì‹œ Blocks ë‚´ë¶€ì—ì„œ ì„ ì–¸í•´ì•¼ í•©ë‹ˆë‹¤
    data_state = gr.State([])

    chat_display = gr.Chatbot(height=700, type="messages")
    txt = gr.Textbox(placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

    # ì±—ë´‡ í•¨ìˆ˜ ì •ì˜
    def chatbot_fn(message, state):
        state = state or []
        state.append({"role":"user","content":message})
        chain = formatted_chain if "ì¶œë ¥" in message else default_chain

        # ì›ë˜ ì“°ì‹œë˜ invoke() ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
        result = chain.invoke({"question": message})
        answer = result.get("answer", "")

        # ì¶œì²˜ í† ê¸€ ì¶”ê°€
        docs = result.get("source_documents", [])
        docs = [d for d in docs if d.metadata.get("bidding_info")]
        if docs:
            sources = "\n".join(f"- {d.metadata['bidding_info']}" for d in docs)
            answer += f"\n<details><summary>ì¶œì²˜ ë‚´ìš© ë³´ê¸°</summary>\n{sources}\n</details>"

        state.append({"role":"assistant","content":answer})
        return state, state, ""  # (chat_display, data_state, txt ìˆœì„œë¡œ ë¦¬í„´)

    # Gradio ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°
    txt.submit(
        fn=chatbot_fn,
        inputs=[txt, data_state],
        outputs=[chat_display, data_state, txt]
    )

    # ì‹¤í–‰
    demo.launch(inbrowser=True)
